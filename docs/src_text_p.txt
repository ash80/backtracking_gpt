Title: SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking

Abstract:
In many domains, autoregressive models can attain high likelihood on the task
of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences proportionally
to their frequency under the data distribution, with no guidance for the
model’s behaviour out of distribution (OOD): leading to compounding error during
autoregressive generation. In order to address this compounding error problem, we
formulate sequence generation as an imitation learning (IL) problem. This allows
us to minimize a variety of divergences between the distribution of sequences
generated by an autoregressive model and sequences from a dataset, including
divergences with weight on OOD generated sequences. The IL framework also
allows us to incorporate backtracking by introducing a backspace action into
the generation process. This further mitigates the compounding error problem by
allowing the model to revert a sampled token if it takes the sequence OOD. Our
resulting method, SequenceMatch, can be implemented without adversarial training
or architectural changes. We identify the SequenceMatch-χ2 divergence as a more
suitable training objective for autoregressive models which are used for generation.
We show that empirically, SequenceMatch training leads to improvements over
MLE on text generation with language models.

Introduction:
Autoregressive models such as the GPT series of causally masked transformers [7, 25] are able to
perform a variety of downstream tasks such as question-answering, translation, and summarization,
after simply training on a large corpus of text with the objective of predicting the next token given
the previous tokens. However, autoregressive language models suffer from a variety of pathological
behavior when deployed on the task of free-form text generation [15, 37], particularly at lower
generation temperatures or with smaller models. These include generating the same token or series
of token repeatedly, or generating gibberish outputs. This phenomenon of degeneration for longer
continuations has also been observed in autoregressive models for video [36]. This problem of neural
text degeneration has been linked to the training objective for LLMs, which trains a conditional
distribution for the next token given a (partial) sentence [10]. When deployed in an autoregressive
fashion, the model has its own outputs as inputs, resulting in a compounding error problem that
rapidly takes the model out of distribution (OOD). This compounding error problem is also a key
issue in the imitation learning subfield of reinforcement learning, where the goal is to learn a
policy (a distribution over next actions given the past) which results in trajectories similar to a set
of provided expert trajectories. The approach of directly matching the expert’s actions leads to